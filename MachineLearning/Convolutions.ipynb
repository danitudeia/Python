{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKoVbE_cr2K1"
   },
   "source": [
    "# Convolutions\n",
    "\n",
    "## Convolutions in NumPy\n",
    "\n",
    "The code below implements a basic *convolution* operation using NumPy arrays; these are the same arrays used in this week's lecture. It uses an input array of 6x6 and a filter of 3x3. Note that the `*` below does elementwise multiplication between two arrays. The example below shows how elementwise matrix multiplication works. (Note: if the arrays do not have the same shape, broadcasting will be done, if possible.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OjVCVdUxr2K3",
    "outputId": "7f520fc6-596e-444a-fc66-14a1d37e0569"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  9],\n",
       "       [16, 25, 36]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr1 = np.array([[1, 2, 3],\n",
    "               [4, 5, 6]])\n",
    "\n",
    "arr2 = np.array([[1, 2, 3],\n",
    "               [4, 5, 6]])\n",
    "\n",
    "arr1 * arr2  # we could also use np.multiply(arr1, arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Et81Fehr2K4",
    "outputId": "346ddacf-6287-43bd-afe4-8df29ce9f8dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(arr1 * arr2) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbyCCKAWr2K4"
   },
   "source": [
    "Execute the code below and make sure you understand the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09CcpiXBr2K4",
    "outputId": "089cd4d4-1c88-42da-901c-e22191633e99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 1, 7, 3, 6, 7],\n",
       "       [3, 2, 2, 8, 6, 1],\n",
       "       [8, 5, 3, 0, 1, 8],\n",
       "       [8, 7, 8, 3, 4, 8],\n",
       "       [5, 4, 7, 3, 6, 4],\n",
       "       [7, 3, 2, 5, 5, 2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[6, 1, 7, 3, 6, 7],\n",
    "             [3, 2, 2, 8, 6, 1],\n",
    "             [8, 5, 3, 0, 1, 8],\n",
    "             [8, 7, 8, 3, 4, 8],\n",
    "             [5, 4, 7, 3, 6, 4],\n",
    "             [7, 3, 2, 5, 5, 2]])\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_oRI-1t1r2K5",
    "outputId": "6881facf-126e-41d4-bbec-aa4c269cffb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 6, 1, 7, 3, 6, 7, 0],\n",
       "       [0, 3, 2, 2, 8, 6, 1, 0],\n",
       "       [0, 8, 5, 3, 0, 1, 8, 0],\n",
       "       [0, 8, 7, 8, 3, 4, 8, 0],\n",
       "       [0, 5, 4, 7, 3, 6, 4, 0],\n",
       "       [0, 7, 3, 2, 5, 5, 2, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pad(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4va8hq6hr2K5",
    "outputId": "37a87a63-ae2e-4ea9-cf5e-016f9414219e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2, 2],\n",
       "       [8, 5, 3],\n",
       "       [8, 7, 8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:4, 0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBFZY3XLr2K5",
    "outputId": "2c5104dd-548c-40af-9766-edccbd2399e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  3,  0],\n",
       "       [-1,  1,  2],\n",
       "       [ 4,  0,  1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = np.array([[0, 3, 0],\n",
    "              [-1, 1, 2],\n",
    "              [4, 0, 1]])\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KosYL7jbr2K5",
    "outputId": "c1568b56-97d8-4072-bbde-32b344f5a0a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.zeros((4, 4))\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVfySRFir2K5"
   },
   "source": [
    "For each line of code below, locate which element of `v` is being updated and which section of the `x` array is involved (use the output for `v` and `x` above for this). Make sure this makes sense based on your understanding of the lecture material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MyZaeG_ar2K6"
   },
   "outputs": [],
   "source": [
    "v[0,0] = np.sum(x[0:3, 0:3] * f) # VERIFY\n",
    "v[0,1] = np.sum(x[0:3, 1:4] * f)\n",
    "v[0,2] = np.sum(x[0:3, 2:5] * f)\n",
    "v[0,3] = np.sum(x[0:3, 3:6] * f)\n",
    "\n",
    "v[1,0] = np.sum(x[1:4, 0:3] * f)\n",
    "v[1,1] = np.sum(x[1:4, 1:4] * f)\n",
    "v[1,2] = np.sum(x[1:4, 2:5] * f)\n",
    "v[1,3] = np.sum(x[1:4, 3:6] * f) # VERIFY\n",
    "\n",
    "v[2,0] = np.sum(x[2:5, 0:3] * f)\n",
    "v[2,1] = np.sum(x[2:5, 1:4] * f)\n",
    "v[2,2] = np.sum(x[2:5, 2:5] * f)\n",
    "v[2,3] = np.sum(x[2:5, 3:6] * f)\n",
    "\n",
    "v[3,0] = np.sum(x[3:6, 0:3] * f)\n",
    "v[3,1] = np.sum(x[3:6, 1:4] * f) # VERIFY\n",
    "v[3,2] = np.sum(x[3:6, 2:5] * f)\n",
    "v[3,3] = np.sum(x[3:6, 3:6] * f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TN1n6_utr2K6",
    "outputId": "bce04302-ae2c-4986-eeaf-490923235d0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41., 57., 40., 26.],\n",
       "       [49., 35., 59., 55.],\n",
       "       [57., 35., 37., 36.],\n",
       "       [64., 50., 30., 45.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAS4KWU7r2K6"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Verify by hand the noted calculations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 1, 7],\n",
       "       [3, 2, 2],\n",
       "       [8, 5, 3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x[0:3, 0:3]\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  3,  0],\n",
       "       [-1,  1,  2],\n",
       "       [ 4,  0,  1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd = (6*0)+(1*3)+(7*0)+(3*-1)+(2*1)+(2*2)+(8*4)+(5*0)+(3*1)\n",
    "hd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ldqq2ux_r2K6"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Keep the same array `x` but change `f` so it is a 5x5 filter (try using a NumPy function to create a random array of integers) to produce a new output `v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 1, 7, 3, 6, 7],\n",
       "       [3, 2, 2, 8, 6, 1],\n",
       "       [8, 5, 3, 0, 1, 8],\n",
       "       [8, 7, 8, 3, 4, 8],\n",
       "       [5, 4, 7, 3, 6, 4],\n",
       "       [7, 3, 2, 5, 5, 2]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3,  0,  3, -1,  3],\n",
       "       [ 3,  0, -3, -5,  0],\n",
       "       [-2, -3, -5,  1,  2],\n",
       "       [-4,  2,  4, -1, -4],\n",
       "       [ 1, -1,  0, -4, -2]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = np.random.randint(-5,5,(5,5))\n",
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = np.zeros((2, 2))\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -91., -100.],\n",
       "       [ -81.,  -43.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2[0,0] = np.sum(x[0:5, 0:5] * f2) # VERIFY\n",
    "v2[0,1] = np.sum(x[0:5, 1:6] * f2)\n",
    "\n",
    "v2[1,0] = np.sum(x[1:6, 0:5] * f2)\n",
    "v2[1,1] = np.sum(x[1:6, 1:6] * f2)\n",
    "\n",
    "v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiagiU-zr2K6"
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Verify by hand some of the calculations for **Exercise 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-91"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand2 = (6*-3)+(1*0)+(7*3)+(3*-1)+(6*3)+(3*3 )+(2*0 )+(2*-3 )+(8*-5) +(6*0) +(8*-2 )+ (5* -3)+(3*-5)+(0*1)+(1*2)+ (8*-4)+(7*2)+(8*4)+(3*-1)+(4*-4)+(5*1)+(4*-1)+(7*0)+(3*-4)+(6*-2)\n",
    "hand2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZr11uZLr2K6"
   },
   "source": [
    "## Convolutions in Keras\n",
    "\n",
    "The code below will allow you to play with a single convolutional layer in Keras. Take a look at the documentation for the [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/) layer, which is also where the original code came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "gK3oCUFsr2K6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o51rtab0r2K6",
    "outputId": "a7123ba1-73ee-4e0b-c689-7a4f0aef82f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 26, 26, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (4, 28, 28, 3)\n",
    "\n",
    "x = tf.random.normal(input_shape)\n",
    "\n",
    "y = Conv2D(filters=2, #number of filters\n",
    "           kernel_size=(3, 3), #size of filter\n",
    "           strides=1, #number of rows/columns that the convolution will slide\n",
    "           padding='valid', # not using padding - border\n",
    "           input_shape=(None, 28, 28, 3))(x) #heigh, weight, number of dimentions/channels (in this case RGB)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U02J83a5r2K6"
   },
   "source": [
    "Here is a brief explanation of the above code:\n",
    "![ ](https://drive.google.com/uc?export=view&id=1BN22UJ_B-dAQV3U9-VHet26T_1LZEQPk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZDy0Vx-r2K6"
   },
   "source": [
    "### Exercises\n",
    "\n",
    "In the code above, make changes to:\n",
    "- input $h$\n",
    "- input $w$\n",
    "- input $n_c$\n",
    "- number of filters\n",
    "- kernel size (same as filter size)\n",
    "- type of padding used\n",
    "- stride value\n",
    "\n",
    "For each change, calculate the dimensions of the output (`y.shape`) by hand, including drawing a diagram (as shown below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 28, 26, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (4, 30, 28, 3)\n",
    "\n",
    "x = tf.random.normal(input_shape)\n",
    "\n",
    "y = Conv2D(filters=2, #number of filters\n",
    "           kernel_size=(3, 3), #size of filter\n",
    "           strides=1, #number of rows/columns that the convolution will slide\n",
    "           padding='valid', # not using padding - border\n",
    "           input_shape=(None, 28, 28, 3))(x) #heigh, weight, number of dimentions/channels (in this case RGB)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 26, 28, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (4, 28, 30, 3)\n",
    "\n",
    "x = tf.random.normal(input_shape)\n",
    "\n",
    "y = Conv2D(filters=2, #number of filters\n",
    "           kernel_size=(3, 3), #size of filter\n",
    "           strides=1, #number of rows/columns that the convolution will slide\n",
    "           padding='valid', # not using padding - border\n",
    "           input_shape=(None, 28, 28, 3))(x) #heigh, weight, number of dimentions/channels (in this case RGB)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 26, 26, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (4, 28, 28, 1)\n",
    "\n",
    "x = tf.random.normal(input_shape)\n",
    "\n",
    "y = Conv2D(filters=2, #number of filters\n",
    "           kernel_size=(3, 3), #size of filter\n",
    "           strides=1, #number of rows/columns that the convolution will slide\n",
    "           padding='valid', # not using padding - border\n",
    "           input_shape=(None, 28, 28, 1))(x) #heigh, weight, number of dimentions/channels (in this case RGB)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2cSYFlGr2K7"
   },
   "source": [
    "<img src=\"Convolution_notebook-1.jpg\" width=600 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uIxf9CXr2K7"
   },
   "source": [
    "## MNIST Revisited\n",
    "\n",
    "Let's now revisit our MNIST. Knowing that the data contains 2-dimensional images of handwritten digits, we should be able to apply what we've learned about convolutions. Thus, in this section, we will create a convolutional neural network (CNN or convnet) for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDWeFJLEr2K7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np # included here for completeness\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwYCXldbr2K7"
   },
   "source": [
    "This time we are going to use a **validation set** to monitor our training progress. We can also use this validation set for *hyperparameter tuning*. Remember, using the validation set allows us to keep the *test set* to gauge how well our final model should do in the real world; that is, the final model only sees the test data once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHcE-zJKr2K7"
   },
   "outputs": [],
   "source": [
    "# Use the first 10,000 samples of our training data as our validation set\n",
    "val_data = train_data[:10000]\n",
    "val_labels = train_labels[:10000]\n",
    "\n",
    "# Use the remainder of the original training data for actual training\n",
    "partial_train_data = train_data[10000:]\n",
    "partial_train_labels = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtrWbp--r2K7"
   },
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9GJp7frr2K7"
   },
   "outputs": [],
   "source": [
    "# Scale the pixel values so they lie in the range of 0-1\n",
    "partial_train_data = partial_train_data / 255.\n",
    "val_data = val_data / 255.\n",
    "test_data = test_data /255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMhi8j1Tr2K7"
   },
   "source": [
    "Note that our data currently has 3 dimensions: `(samples, height, width)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yNVdbY2r2K7"
   },
   "outputs": [],
   "source": [
    "print(partial_train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60YxS3wKr2K7"
   },
   "source": [
    "Our convolutional neural network will expect 4-dimensional data: `(batch_size, height, width, channels)`. Note that depending on how you decide to update the parameters of the network, `batch_size` could equal the number of `samples` (as in *batch gradient descent*), or it could equal a single sample (as in *stochastic gradient descent*, or it can equal the batch size (as in *mini-batch gradient descent*).\n",
    "\n",
    "We can use a NumPy function to add this dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbkNFX4Ir2K7"
   },
   "outputs": [],
   "source": [
    "partial_train_data = np.expand_dims(partial_train_data, axis=3)\n",
    "val_data = np.expand_dims(val_data, axis=3)\n",
    "test_data = np.expand_dims(test_data, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHZmBny6r2K7"
   },
   "outputs": [],
   "source": [
    "print(partial_train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9trMaMeJr2K7"
   },
   "source": [
    "Note how a fourth dimension was added to our data. This dimension corresponds to the number of channels in our input data. Here it is 1, since the images are all greyscale. It would be 3 if the images were RGB. Also note, that the convention here is *channels last*, as opposed to *channels first*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBnz9x-Br2K7"
   },
   "source": [
    "As in Lab 1, we need to convert our label data to the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RPirclrr2K7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "partial_train_labels = to_categorical(partial_train_labels)\n",
    "val_labels = to_categorical(val_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61wvYSKsr2K7"
   },
   "source": [
    "We will now import the necessary modules for building our convolutional neural network. Since we are using Keras's sequential API we need to import the `Sequential` module. The remaining 3 imports will help us build the layers of our CNN. `Conv2D` creates the convolutional layers we have been discussing in the lectures. `Flatten` is used to create a 1 dimensional vector so we can feed the output of our convolutional layers to the fully-connected layers. We used NumPy's `reshape` function to do this flattening in Lab 1. And the `Dense` layer is the same as what we used in Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ve1vOFOwr2K7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdbuV_POr2K7"
   },
   "source": [
    "We are going to use a slightly different approach to building our network than we did in Lab 1. Here we will directly add a *list of layers* to the `Sequential()` object. That is, we put all our layers inside square brackets `[...]` and put this inside the `Sequential( [...] )` object to create our model. In Lab 1 we used the `.add()` method to add individual layers to our `Sequential()` object that we initialized without any layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Oyav2P8r2K7"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='same',\n",
    "           activation='relu',\n",
    "           input_shape=(28, 28, 1)),\n",
    "    Conv2D(filters=32,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=2,\n",
    "           padding='valid',\n",
    "           activation='relu'),\n",
    "    Conv2D(filters=64,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=1,\n",
    "           padding='same',\n",
    "           activation='relu'),\n",
    "    Conv2D(filters=64,\n",
    "           kernel_size=(3, 3),\n",
    "           strides=2,\n",
    "           padding='valid',\n",
    "           activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhtSLfYvr2K_"
   },
   "source": [
    "It is often helpful to see the tensor shapes and number of parameters per layer. We can get this information by using the `.summary()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOg4N1qDr2K_"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZV5Kqkp2r2K_"
   },
   "source": [
    "We are still tackling the same type of problem (multi-class classification) so the same loss and metrics will work for us here. The optimizer `rmsprop` is the same as we used before and can be taken as the default method (or recipe) to try out for updating the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enNkhneGr2K_"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA8_DPq9r2K_"
   },
   "source": [
    "We now fit our model to the remaining training data (the original training data minus the validation data). You will now see that *loss* and *accuracy* get updated for each batch of images (here set to 256) but the *validation loss* and *validation accuracy* get updated after each *epoch*. Note that the *validation data* is not being used to train the model. Each batch of the training data is used to update the parameters and then, once we have gone through all of the samples in our training data (that is, all the samples in `partial_train_data`) the model is used to make predictions for the validation set. From those predictions the validation loss and accuracy are calculated.\n",
    "\n",
    "Each epoch of training should take 30-50s to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPwyne7_r2K_"
   },
   "outputs": [],
   "source": [
    "history = model.fit(partial_train_data,\n",
    "                    partial_train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val_data, val_labels),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJ1_Izu2r2K_"
   },
   "source": [
    "The values for the training loss and accuracy, as well as the validation loss and accuracy, are stored in the `history` variable. You can see the structure of the dictionary that stores this information as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63nzLyber2K_"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM7_DQ46r2K_"
   },
   "source": [
    "We will now use this information to visualize the progress our network makes on the loss and accuracy as the number of epochs increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hy9jC0tVr2K_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # needed to create our plot\n",
    "\n",
    "history_dict = history.history # the dictionary that has the information on loss and accuracy per epoch\n",
    "\n",
    "loss_values = history_dict['loss']   # training loss\n",
    "val_loss_values = history_dict['val_loss'] # validation loss\n",
    "\n",
    "epochs = range(1, len(loss_values)+1)  #creates list of integers to match the number of epochs of training\n",
    "\n",
    "# code to plot the results\n",
    "plt.plot(epochs, loss_values, 'b', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss_values, 'r', label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HScYjSMcr2K_"
   },
   "outputs": [],
   "source": [
    "# As above, but this time we want to visualize the training and validation accuracy\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'b', label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc_values, 'r', label=\"Validation Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMpZ-gRQr2LA"
   },
   "source": [
    "### Exercise: Parameters\n",
    "\n",
    "For the model we have created, calculate the number of parameters by hand for each layer and compare to the output of `model.summary()`. For Exercise 7 in Lab 1, how many parameters did your model have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAy9FvfHAzYr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Du5wGiMr2LA"
   },
   "source": [
    "## Exercise: Change the layers\n",
    "\n",
    "Play around with the **number of filters** and the **filter size**, **strides**, **padding** in our model. Note the change in:\n",
    "- number of parameters in the model\n",
    "- training and validation losses and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPj3etUBAy35"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
