{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25762608-8da6-45d8-a60e-3d66cd80afc9",
   "metadata": {},
   "source": [
    "### Sampling and point estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807f455-add8-4950-b194-d4eef6b2a69b",
   "metadata": {},
   "source": [
    "##### Use np.random.choice() to select an initial_row_number from a range of values that begins with zero and ends with the length of btc_sp_df, excluding the last 90 rows.\n",
    "##### Select rows from initial_row_number to initial_row_number + 90 using .iloc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a0154-1028-46f1-ac60-401dde569ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random starting row number, not including the last 90 rows\n",
    "initial_row_number = np.random.choice(range(0,(len(btc_sp_df) - 90)))\n",
    "\n",
    "# Use initial_row_number to select the next 90 rows from that row number\n",
    "sample_df = btc_sp_df.iloc[initial_row_number:(initial_row_number + 90)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b9196-8e63-46cd-a057-bb98840f78ad",
   "metadata": {},
   "source": [
    "##### Compute the percentage increase of SP500 over this 90-day period using the Close_SP500 column of your sample_df.\n",
    "##### Similarly, compute the percentage increase of BTC over this 90-day period using the Close_BTC column of your sample_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17a10b-4fc8-45c0-9589-52f1ed5061e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random starting row number, not including the last 90 rows\n",
    "initial_row_number = np.random.choice(range(btc_sp_df.shape[0] - 90))\n",
    "\n",
    "# Use initial_row_number to select the next 90 rows from that row number\n",
    "sample_df = btc_sp_df.iloc[initial_row_number:(initial_row_number + 90)]\n",
    "\n",
    "# Use sample_df to compute the percent increase in Close_SP500\n",
    "sp500_pct_change = (sample_df.iloc[0]['Close_SP500'] - sample_df.iloc[-1]['Close_SP500']) / sample_df.iloc[0]['Close_SP500']\n",
    "\n",
    "# Use sample_df to compute the percent increase in Close_BTC\n",
    "btc_pct_change = (sample_df.iloc[0]['Close_BTC'] - sample_df.iloc[-1]['Close_BTC']) / sample_df.iloc[0]['Close_BTC']\n",
    "\n",
    "print('SP500: ', sp500_pct_change, '\\n', 'BTC: ', btc_pct_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd41b89-0c08-4153-9612-ee9c225566dd",
   "metadata": {},
   "source": [
    "### Repeated sampling, point estimates and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b501e3a9-8ec3-4a79-b005-40f00aa3a241",
   "metadata": {},
   "source": [
    "##### Write a for loop that runs ten times.\n",
    "##### Inside the loop, select a random starting row number, avoiding the last 90 row numbers.\n",
    "##### Select the ninety rows after the starting row.\n",
    "##### Using your sample_df, compute the percent change of BTC closing price using the Close_BTC column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbcb320-46f8-419a-8bde-8edbdcd90fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a for loop which repeats the sampling ten times\n",
    "for i in range(10):\n",
    "    # Select a random starting row number\n",
    "    initial_row_number = np.random.choice(range(btc_sp_df.shape[0] - 90))\n",
    "    # Select the next 90 rows after the starting row\n",
    "    sample_df = btc_sp_df[initial_row_number:initial_row_number + 90]\n",
    "    # Compute the percent change in closing price of BTC and print it\n",
    "    btc_pct_change = (sample_df.iloc[0]['Close_BTC'] - sample_df.iloc[-1]['Close_BTC']) / sample_df.iloc[0]['Close_BTC']\n",
    "    print(btc_pct_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97113a1d-5577-41c9-a057-94288f9afbc7",
   "metadata": {},
   "source": [
    "### Visualizing samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553fcef-027d-4cee-9f9c-7a51e6a9ec64",
   "metadata": {},
   "source": [
    "##### Plot a histogram of BTC percent change with 15 bins with the y-values displaying the density, rather than the count.\n",
    "##### Set the x-axis label to \"BTC 90-day percent change\".\n",
    "##### Set the y-axis label to \"Percent of samples\".\n",
    "##### Set the title to \"Sampling distribution of BTC 90-day change\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5cb37-66e1-42dc-8ba2-ae9a4362d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of percent changes\n",
    "plt.hist(btc_pct_change_list, bins=15, density=True)\n",
    "# Set the x-axis label\n",
    "plt.xlabel('BTC 90-day percent change')\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Percent of samples')\n",
    "# Set the title\n",
    "plt.title('Sampling distribution of BTC 90-day change')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c40752-01bc-4ab7-80d2-c000334c7d66",
   "metadata": {},
   "source": [
    "### Normal sampling distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad844445-ae37-4d86-8413-e338abb333de",
   "metadata": {},
   "source": [
    "##### Define a variable num_samples as the desired number of samples (200), and define an empty list sample_means to store the mean from each of the 200 samples.\n",
    "##### Write a for loop which will repeat the sampling process num_samples times.\n",
    "##### Select 500 random S&P500 closing prices from the Close_SP500 column of btc_sp_df.\n",
    "##### Compute the mean of each of these samples and store them in sample_means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78dbcf-c012-42a6-ac0a-2b43d21fbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of samples to take and store the sample means\n",
    "num_samples = 200\n",
    "sample_means = []\n",
    "\n",
    "# Write a for loop which repeats the sampling num_samples times\n",
    "for i in range(num_samples):\n",
    "  # Select 500 random Close_SP500 prices \n",
    "  sp500_sample = np.random.choice(btc_sp_df['Close_SP500'], size=500)\n",
    "  # Compute mean closing price and save it to sample_means\n",
    "  sample_means.append(sp500_sample.mean())\n",
    "    \n",
    "plt.hist(sample_means)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7adce-869a-4ecb-8cd7-9ede8fb8f53c",
   "metadata": {},
   "source": [
    "### Calculating confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd5a2f-4b08-47ba-8048-f92fa801d319",
   "metadata": {},
   "source": [
    "##### Select a random sample of 500 days from the column Close_SP500.\n",
    "##### Calculate the mean of this random sample.\n",
    "##### Calculate the standard error of this random sample as the standard deviation divided by the square root of the sample size.\n",
    "##### Calculate a 95% confidence interval using the values you just calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cbedb-dc97-4a0a-b8f1-7b447941f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample of 500 random days\n",
    "sample_closing = np.random.choice(btc_sp_df['Close_SP500'], size=500)\n",
    "\n",
    "# Calculate the mean of the sample\n",
    "sample_mean = sample_closing.mean()\n",
    "\n",
    "# Calculate the standard error of the sample\n",
    "sample_se = sample_closing.std() / np.sqrt(sample_closing.shape[0])\n",
    "\n",
    "# Calculate a 95% confidence interval using this data\n",
    "stats.norm.interval(alpha=0.95,\n",
    "                   loc=sample_mean,\n",
    "                   scale=sample_se)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba7ccf-cd68-47f1-9a46-ebc0e9f91386",
   "metadata": {},
   "source": [
    "### Drawing conclusions from samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c4dbb-1eb7-4a38-9e19-fdc9e776b7e0",
   "metadata": {},
   "source": [
    "##### Randomly select 500 rows from btc_sp_df.\n",
    "##### Use stats.norm.interval() to create a 95% confidence interval for Close_SP500 column of your sampled DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf2fd6-d28f-4c3d-a615-9a3b01d685a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1: Select a random sample of 500 rows\n",
    "sample_df = btc_sp_df.sample(n=500)\n",
    "\n",
    "# Compute a 95% confidence interval for the closing price of SP500\n",
    "sample_ci = stats.norm.interval(alpha=0.95, \n",
    "                            loc=sample_df['Close_SP500'].mean(), \n",
    "                            scale=sample_df['Close_SP500'].std()/np.sqrt(500))\n",
    "\n",
    "print(sample_ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec4854-3840-48a6-81ae-e381981f037c",
   "metadata": {},
   "source": [
    "##### Select the first 500 rows from btc_sp_df.\n",
    "##### Compute the 95% confidence interval again using first_500_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d146a-39fe-4993-93e5-0aa17c96b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1: Select a random sample of 500 rows\n",
    "sample_df = btc_sp_df.sample(n=500)\n",
    "\n",
    "sample_ci = stats.norm.interval(alpha=0.95, loc=sample_df['Close_SP500'].mean(), scale=sample_df['Close_SP500'].std()/np.sqrt(500))\n",
    "\n",
    "# Sample 2: Select the first 500 rows\n",
    "first_500_df = btc_sp_df.iloc[:500]\n",
    "\n",
    "# Compute a 95% confidence interval for the closing price of SP500\n",
    "first_500_ci = stats.norm.interval(alpha=0.95, loc=first_500_df['Close_SP500'].mean(), scale=first_500_df['Close_SP500'].std()/np.sqrt(500))\n",
    "\n",
    "print(first_500_ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3b7b8d-57dc-469c-b9dc-d6719cd62a36",
   "metadata": {},
   "source": [
    "### Testing for normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613622ec-8678-490e-8b0a-25db50778e6c",
   "metadata": {},
   "source": [
    "##### Plot a histogram showing the Years of Employment for the employees.\n",
    "##### Conduct an Anderson-Darling test for normality to see if Years of Employment is approximately normally distributed.\n",
    "##### Find which critical_values the test statistic is greater than.\n",
    "##### Print the significance_level(s) at which the null hypothesis would be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469bb4d-4a52-4692-9c96-f0af8416fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the employees' \"Years of Employment\"\n",
    "salary_df['Years of Employment'].plot(kind=\"hist\")\n",
    "plt.show()\n",
    "\n",
    "# Conduct an Anderson-Darling test using the years of employment from salary_df\n",
    "result = stats.anderson(salary_df['Years of Employment'])\n",
    "\n",
    "# Print which critical values the test statistic is greater than the critical values\n",
    "print(result.statistic > result.critical_values)\n",
    "\n",
    "# Print the significance levels at which the null hypothesis is rejected\n",
    "print(result.significance_level[result.statistic > result.critical_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7973bebe-5265-4b01-b6f5-ab049ee1cb0c",
   "metadata": {},
   "source": [
    "### Distribution of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c2a76-bedf-403b-95cd-89f744ad1db2",
   "metadata": {},
   "source": [
    "##### Compute the error as the actual salaries minus the predicted salaries.\n",
    "##### Plot the errors in a histogram.\n",
    "##### Conduct an Anderson-Darling test of normality for the errors.\n",
    "##### Find and print the significance_level(s) at which the null hypothesis would be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e33b2-7b7f-4af7-aa42-fb8fe4861298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the error as actual minus predicted salary\n",
    "error = salaries - preds\n",
    "\n",
    "# Plot the errors as a histogram\n",
    "plt.hist(error)\n",
    "plt.show()\n",
    "\n",
    "# Conduct an Anderson-Darling test using the years of experience\n",
    "result = stats.anderson(error)\n",
    "\n",
    "# Find where the result is significant\n",
    "print(result.significance_level[result.statistic > result.critical_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed53ec4-b95d-4764-becb-32f33cc3e8b0",
   "metadata": {},
   "source": [
    "### Fitting a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c88afc-7643-445b-b415-0a206a25cb6b",
   "metadata": {},
   "source": [
    "##### Fit a normal distribution to the Years of Employment column and save the resulting mean and standard deviation.\n",
    "##### Use this mean and standard deviation in a normal CDF to estimate the percentage of employees with less than ten years of experience.\n",
    "##### Print out this percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad4d26-6d1c-49ac-b75a-3095c2cb05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a normal distribution to the data\n",
    "mu, std = stats.norm.fit(salary_df['Years of Employment'])\n",
    "\n",
    "# Compute the percentage of employees with less than 10 years experience\n",
    "percent = stats.norm.cdf(10, loc=mu, scale=std)\n",
    "\n",
    "# Print out this percentage\n",
    "print(percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf4d46-b5fa-4ada-b221-2763688425cc",
   "metadata": {},
   "source": [
    "### Testing for correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137122a-9414-4cce-bcd0-3e1dc3d8a13c",
   "metadata": {},
   "source": [
    "##### Create a line graph with two lines, one for houston_rents and one for lasvegas_rents, using the dates on the x-axis.\n",
    "##### Compute the Pearson correlation coefficient and its associated p-value.\n",
    "##### Determine and print out a Boolean that tells you whether the p-value is significant at the 5% level.\n",
    "##### Print out R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476fd79a-8121-42ea-93a7-e51e03bd174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line graph showing the rents for both San Francisco and Las Vegas\n",
    "plt.plot(dates, houston_rents, label='Houston')\n",
    "plt.plot(dates, lasvegas_rents, label='Las Vegas')\n",
    "plt.show()\n",
    "\n",
    "# Compute the Pearson correlation coefficient R, as well as the p-value\n",
    "r, p_value = stats.pearsonr(houston_rents, lasvegas_rents)\n",
    "\n",
    "# Print if the p-value is less than alpha = 5%\n",
    "print(p_value < 0.05)\n",
    "\n",
    "# Print out R-squared\n",
    "print(r**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb8430-fa9a-465d-8ca8-f1494d9cc483",
   "metadata": {},
   "source": [
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b9302-8f4e-4caa-9849-c26d2daac8f4",
   "metadata": {},
   "source": [
    "##### Select all but the first twelve rents (to exclude the first year) from la_rents.\n",
    "##### Select all but the last twelve rents (to exclude the last year) from la_rents.\n",
    "##### Compute the Pearson correlation coefficient R between these two lists.\n",
    "##### Check if the p-value is significant at the 5% level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bacf898-790e-4a6a-a4fd-008ad9e23213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all but the first twelve rents\n",
    "la_rents_initial = la_rents[12:]\n",
    "\n",
    "# Select all but the last twelve rents (12 month lag)\n",
    "la_rents_lag = la_rents[:-12]\n",
    "\n",
    "# Compute the correlation between the initial values and the lagged values\n",
    "r, p_value = stats.pearsonr(la_rents_initial,la_rents_lag)\n",
    "\n",
    "# Check if the p-value is significant at the 5% level\n",
    "print(p_value < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7debf1fe-e345-43e5-b63c-9b70cd80c4f7",
   "metadata": {},
   "source": [
    "### Explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b82fc-62f4-442b-9948-614729e9ab9c",
   "metadata": {},
   "source": [
    "##### Compute the Pearson correlation coefficient between houston_rents and lasvegas_rents.\n",
    "##### Print the square of the correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd04e73-3807-4fa3-83b6-296033f2cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation between Houston and Las Vegas\n",
    "r, p_value = stats.pearsonr(houston_rents, lasvegas_rents)\n",
    "\n",
    "# Print R-squared\n",
    "print(r**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6283d-94c3-45d5-820e-d3031381cb5c",
   "metadata": {},
   "source": [
    "### Equal variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f91892a-f528-45f7-8442-88b12c13e91a",
   "metadata": {},
   "source": [
    "##### Select the funding for each market individually from investments_df using the column names given.\n",
    "##### Conduct Levene tests for equal variance between each pair of industries, in the following order: (i) Biotechnology and Enterprise Software, (ii) Biotechnology and Health Care, and (iii) Enterprise Software and Health Care, corresponding to statistic1, statistic2, and statistic3, respectively.\n",
    "##### In each case, return a Boolean that indicates whether the null hypothesis of equal variance is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7b53a-40e6-4a7a-9618-615a610dadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select each industry separately\n",
    "biotech_df = investments_df[(investments_df['market'] == 'Biotechnology')]\n",
    "enterprise_df = investments_df[(investments_df['market'] == 'Enterprise Software')]\n",
    "health_df = investments_df[(investments_df['market'] == 'Health Care')]\n",
    "\n",
    "# Conduct Levene tests for equal variance between funding_total_usd for all pairs of industries\n",
    "statistic_1, p_value_1 = stats.levene(biotech_df['funding_total_usd'],enterprise_df['funding_total_usd'])\n",
    "statistic_2, p_value_2 = stats.levene(biotech_df['funding_total_usd'],health_df['funding_total_usd'])\n",
    "statistic_3, p_value_3 = stats.levene(enterprise_df['funding_total_usd'],health_df['funding_total_usd'])\n",
    "\n",
    "# Print if the p-value is significant at the 5% level\n",
    "print(p_value_1 < 0.05)\n",
    "print(p_value_2 < 0.05)\n",
    "print(p_value_3 < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c5c38-b64c-4dd3-aa3c-6b4383df11ca",
   "metadata": {},
   "source": [
    "### Normality of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd474a5f-fb97-42ff-934d-a20a6565413e",
   "metadata": {},
   "source": [
    "##### Plot a histogram with 33% transparency for each industry's funding_total_usd in the following order: Biotechnology, then Enterprise Software, and then E-commerce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1a871-888f-4563-9bae-adec196f66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the funding for each industry\n",
    "biotech_df['funding_total_usd'].plot(kind='hist', alpha=0.33)\n",
    "enterprise_df['funding_total_usd'].plot(kind='hist', alpha=0.33)\n",
    "ecommerce_df['funding_total_usd'].plot(kind='hist', alpha=0.33)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847a1f2-d90c-4621-a047-464d139cae93",
   "metadata": {},
   "source": [
    "##### Make a histogram of the log-transform each industry's funding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce349930-be9d-4890-b7dd-050cd65b2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the log funding for each industry\n",
    "np.log(biotech_df['funding_total_usd']).plot(kind='hist', color='blue', alpha=0.33)\n",
    "np.log(enterprise_df['funding_total_usd']).plot(kind='hist', color='green', alpha=0.33)\n",
    "np.log(ecommerce_df['funding_total_usd']).plot(kind='hist', color='red', alpha=0.33)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1419dd-8216-443c-bb28-2230d16d6e82",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a533d1-88a1-4606-b6ba-03bdd6de055e",
   "metadata": {},
   "source": [
    "##### Conduct a one-way ANOVA test using each of the three log-transformed fundings in the following order of arguments: Biotechnology, Enterprise Software, Health Care.\n",
    "##### Print out if the p-value is significant at 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc7f50-1af5-496b-b427-b4e7216e05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "biotech_log_funding = np.log(biotech_df['funding_total_usd'])\n",
    "enterprise_log_funding = np.log(enterprise_df['funding_total_usd'])\n",
    "health_log_funding = np.log(health_df['funding_total_usd'])\n",
    "\n",
    "# Conduct a one-way ANOVA test to compare the log-funding\n",
    "s, p_value = stats.f_oneway(biotech_log_funding, enterprise_log_funding,health_log_funding)\n",
    "\n",
    "# Print if the p-value is significant at 5%\n",
    "print(p_value < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbddbe7-65f2-4d83-a6c9-1a8dd53f8136",
   "metadata": {},
   "source": [
    "### Comparing rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32deedd-bde5-4472-b0c3-a25c2c45f8a2",
   "metadata": {},
   "source": [
    "##### Compute Kendall's tau rank correlation coefficient between columns thew_rank and arw_rank.\n",
    "##### Compute Kendall's tau between the thew_rank and cw_rank columns.\n",
    "##### Compute Kendall's tau between the arw_rank and cw_rank columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b178da35-48ee-4646-ade6-f5298b6d4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Kendall's tau between the THEW and ARW rankings\n",
    "tau_thew_arw, p_value_thew_arw = stats.kendalltau(rankings_df['thew_rank'], rankings_df['arw_rank'])\n",
    "\n",
    "# Compute Kendall's tau between the THEW and CW rankings\n",
    "tau_thew_cw, p_value_thew_cw = stats.kendalltau(rankings_df['thew_rank'], rankings_df['cw_rank'])\n",
    "\n",
    "# Compute Kendall's tau between the ARW and CW rankings\n",
    "tau_arw_cw, p_value_arw_cw = stats.kendalltau(rankings_df['cw_rank'], rankings_df['arw_rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f85134-8c24-44d4-af72-f479b26010a7",
   "metadata": {},
   "source": [
    "### Comparing medians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf601661-d2f6-4018-a4ec-341007308e1e",
   "metadata": {},
   "source": [
    "##### Plot a histogram of the cw_score column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38173ed-6c0a-4142-9b87-288981fa95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the CW total score\n",
    "rankings_df['cw_score'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893725f-cf1c-4322-b45f-4cbcdd007cd2",
   "metadata": {},
   "source": [
    "##### Plot a histogram of the arw_score column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73c715-47e6-4e89-a14e-089fd893a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the CW total score\n",
    "rankings_df['cw_score'].hist()\n",
    "plt.show()\n",
    "\n",
    "# Plot a histogram of the ARW total score\n",
    "rankings_df['arw_score'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89528a90-6701-42a9-a746-bdeeb89a4f66",
   "metadata": {},
   "source": [
    "##### Conduct a Mood's median test to test if the median scores for ARW and CW are equal.\n",
    "##### Print out a Boolean value that indicates whether the p-value is less than the significance level of 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60f8ff-c094-4c15-b641-00cd4dcd9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the CW total score\n",
    "rankings_df['cw_score'].hist()\n",
    "plt.show()\n",
    "\n",
    "# Plot a histogram of the ARW total score\n",
    "rankings_df['arw_score'].hist()\n",
    "plt.show()\n",
    "\n",
    "# Conduct a Mood's median test comparing cw_score and arw_score\n",
    "s, p_value, med, table = stats.median_test(rankings_df['cw_score'], rankings_df['arw_score'])\n",
    "\n",
    "# Check if the p-value is significant at 5%\n",
    "print(p_value < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30658c6-d859-4488-93b4-53208b348ee7",
   "metadata": {},
   "source": [
    "### Effect size for means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2952de-7a91-4ee9-a35d-79c634c5c0c6",
   "metadata": {},
   "source": [
    "##### Filter investments_df to select funding_rounds 1 and 2 separately.\n",
    "##### Calculate the standard deviation and sample size of each round.\n",
    "##### Calculate the pooled standard deviation between the two rounds.\n",
    "##### Calculate Cohen's d using the terms you just calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6ad41-257b-469e-b54d-f52933d46499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all investments from rounds 1 and 2 separately\n",
    "round1_df = investments_df[investments_df['funding_rounds'] == 1]\n",
    "round2_df = investments_df[investments_df['funding_rounds'] == 2]\n",
    "\n",
    "# Calculate the standard deviation of each round and the number of companies in each round\n",
    "round1_sd = round1_df['funding_total_usd'].std()\n",
    "round2_sd = round2_df['funding_total_usd'].std()\n",
    "round1_n = round1_df.shape[0]\n",
    "round2_n = round2_df.shape[0]\n",
    "\n",
    "# Calculate the pooled standard deviation between the two rounds\n",
    "pooled_sd = np.sqrt(((round1_n - 1) * round1_sd ** 2 + (round2_n - 1) * round2_sd ** 2) / (round1_n + round2_n - 2))\n",
    "\n",
    "# Calculate Cohen's d\n",
    "d = (round1_df['funding_total_usd'].mean() - round2_df['funding_total_usd'].mean()) / pooled_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7229cbc-c44d-4dde-83aa-55c1ad6777a0",
   "metadata": {},
   "source": [
    "### Effect size for correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd97f2d-0c29-4020-8502-186d65f22f6e",
   "metadata": {},
   "source": [
    "##### Compute the volatility of BTC.\n",
    "##### Repeat for the S&P 500.\n",
    "##### Compute between the volatility of each asset.\n",
    "##### Compute between the volatility and closing price of BTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bbccf-ed17-4738-b4aa-37681361fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the volatility of Bitcoin\n",
    "btc_sp_df['Volatility_BTC'] = (btc_sp_df['High_BTC'] - btc_sp_df['Low_BTC']) / btc_sp_df['Close_BTC']\n",
    "\n",
    "# Compute the volatility of the S&P500\n",
    "btc_sp_df['Volatility_SP500'] = (btc_sp_df['High_SP500'] - btc_sp_df['Low_SP500']) / btc_sp_df['Close_SP500']\n",
    "\n",
    "# Compute and print R^2 between the volatility of BTC and SP500\n",
    "r_volatility, p_value_volatility = stats.pearsonr(btc_sp_df['Volatility_BTC'], btc_sp_df['Volatility_SP500'])\n",
    "print('R^2 between volatility of the assets:', r_volatility**2)\n",
    "\n",
    "# Compute and print R^2 between the volatility of BTC and the closing price of BTC\n",
    "r_closing, p_value_closing = stats.pearsonr(btc_sp_df['Volatility_BTC'], btc_sp_df['Close_BTC'])\n",
    "print('R^2 between closing price and volatility of BTC:', r_closing**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb305e-deb8-4593-b663-6286d111b15f",
   "metadata": {},
   "source": [
    "### Effect size for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a404beb-86e2-4533-af3d-2590611a95d3",
   "metadata": {},
   "source": [
    "##### Compute the chi-squared statistic from the contingency table employees_df.\n",
    "##### Compute the degrees of freedom for Cramer's V.\n",
    "##### Compute the total number of people in the contingency table.\n",
    "##### Compute Cramer's V using the equation from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d1229-5761-4775-a0a4-a8ebbf6c4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the chi-squared statistic\n",
    "chi2, p, d, expected = stats.chi2_contingency(employees_df)\n",
    "\n",
    "# Compute the DOF using the number of rows and columns\n",
    "dof = min(employees_df.shape[0] - 1, employees_df.shape[1] - 1)\n",
    "\n",
    "# Compute the total number of people\n",
    "n = np.sum(employees_df.values)\n",
    "\n",
    "# Compute Cramer's V\n",
    "v = np.sqrt((chi2 / n) / dof)\n",
    "\n",
    "print(\"Cramer's V:\", v, \"\\nDegrees of freedom:\", dof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580433ea-a3de-4419-a7ab-1a6d74f1d7bd",
   "metadata": {},
   "source": [
    "### Multiple comparisons problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582198d-f8ea-4710-95ea-6d764d370497",
   "metadata": {},
   "source": [
    "##### Store the number of people in the dataset in n_rows (each row is a person), and initialize the number of significant results, n_significant, to zero.\n",
    "##### Write a for loop which runs 1000 times and generates n_rows random numbers.\n",
    "##### Compute Pearson's R and the associated p-value between these randomly generated numbers and the police officer salaries.\n",
    "##### If the p-value is significant at 5%, add one to n_significant using the += operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273de08-2d37-40cf-9543-b84a7a8082da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of rows and initialize n_significant\n",
    "n_rows = len(police_salaries_df)\n",
    "n_significant = 0\n",
    "\n",
    "# For loop which generates n_rows random numbers 1000 times\n",
    "for i in range(1000):\n",
    "  random_nums = np.random.uniform(size=n_rows)\n",
    "  # Compute correlation between random_nums and police salaries\n",
    "  r, p_value = stats.pearsonr(police_salaries_df['Annual Salary'], random_nums)\n",
    "  # If the p-value is significant at 5%, increment n_significant\n",
    "  if p_value < 0.05:\n",
    "    n_significant += 1\n",
    "    \n",
    "print(n_significant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea3386-c198-4551-8e5c-a17d0249fbca",
   "metadata": {},
   "source": [
    "### Bonferonni-Holm correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c675c-ab11-43f0-a890-ac7503e712db",
   "metadata": {},
   "source": [
    "##### Compute the Bonferonni-corrected value of alpha = 5%.\n",
    "##### Print out how many of the p-values were less than this corrected cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27df91d-cb2b-4bdc-823e-12d82df640fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Bonferonni-corrected alpha\n",
    "bonf_alpha = 0.05/p_values\n",
    "\n",
    "# Check how many p-values were significant at this level\n",
    "print(sum(p_values < bonf_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90680b-1471-4009-97a7-8c69232716e6",
   "metadata": {},
   "source": [
    "### Computing power and sample sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658e35a-8e64-40e0-9c1d-dfed6a9af977",
   "metadata": {},
   "source": [
    "##### Compute the ratio of games companies (games_n) to advertising companies (ads_n).\n",
    "##### Compute power with an effect size ads_games_cohensd, alpha of 5%, number of advertising companies ads_n as nobs1, and the games_ads_ratio above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41baa4f0-db27-450b-90e3-ee969f39dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ratio of games to advertising companies\n",
    "games_ads_ratio = games_n / ads_n\n",
    "\n",
    "# Compute the power of the test\n",
    "TTestIndPower().power(effect_size=ads_games_cohensd, \n",
    "            nobs1=ads_n,\n",
    "            alpha=0.05,\n",
    "            ratio=games_ads_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d933eb7-ed64-4a67-850e-274be73834fc",
   "metadata": {},
   "source": [
    "##### Solve for the sample size nobs1 needed to achieve a power of 80% with an alpha of 5% and an effect size of ads_games_cohensd by setting nobs1 equal to None in solve_power().\n",
    "##### Print nobs1, the number of observations in one group (e.g. ads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fad63e-28e8-4301-a750-27e5abc29a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ratio of games to advertising companies\n",
    "games_ads_ratio = games_n / ads_n\n",
    "\n",
    "TTestIndPower().power(effect_size=ads_games_cohensd, \n",
    "                      nobs1=ads_n,\n",
    "                      alpha=0.05,\n",
    "                      ratio=games_ads_ratio)\n",
    "\n",
    "# Solve for the sample size needed to achieve a power of 80%\n",
    "nobs1 = TTestIndPower().solve_power(effect_size=ads_games_cohensd, \n",
    "                      nobs1=None,\n",
    "                      alpha=0.05,\n",
    "                      power=0.8)\n",
    "\n",
    "# Print the number of participants needed in one group\n",
    "print(nobs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36bd1e-ff29-461a-b139-de453d3e2214",
   "metadata": {},
   "source": [
    "### Bootstrap confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f64e52-6506-46e7-9da8-79b402f66f5a",
   "metadata": {},
   "source": [
    "##### Compute the daily percent change of BTC and SP500; use the console to see the columns needed.\n",
    "##### Write a function which computes Pearson's R and only returns R (not the p-value).\n",
    "##### Form a bootstrap confidence interval using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cbb20-cd92-4598-826e-9bad8797fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the daily percent change of each asset\n",
    "btc_sp_df['Pct_Daily_Change_BTC'] = (btc_sp_df['Open_BTC'] - btc_sp_df['Close_BTC']) / btc_sp_df['Open_BTC']\n",
    "btc_sp_df['Pct_Daily_Change_SP500'] = (btc_sp_df['Open_SP500'] - btc_sp_df['Close_SP500']) / btc_sp_df['Open_SP500']\n",
    "\n",
    "# Write a function which returns the correlation coefficient\n",
    "def pearson_r(x, y):\n",
    "    return stats.pearsonr(x, y)[0]\n",
    "  \n",
    "# Compute a bootstrap confidence interval\n",
    "ci = stats.bootstrap((btc_sp_df['Pct_Daily_Change_BTC'], btc_sp_df['Pct_Daily_Change_SP500']), \n",
    "                     statistic=pearson_r, \n",
    "                     vectorized=False, paired=True, n_resamples=1000, random_state=1)\n",
    "\n",
    "print(ci.confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0210239c-17a5-41cf-b489-5b7b215705cc",
   "metadata": {},
   "source": [
    "### Bootstrapping vs. normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec39759-50f3-4969-b3b3-2bd608f18f5f",
   "metadata": {},
   "source": [
    "##### Select only the companies in the Analytics market.\n",
    "##### Construct a 95% confidence interval for the mean private_equity using the confidence interval function from stats.norm.\n",
    "##### Do the same calculation on private_equity, but using a bootstrap confidence interval instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376486ad-9caa-4cdf-8f9a-9b138800b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select just the companies in the Analytics market\n",
    "analytics_df = investments_df[investments_df['market'] == 'Analytics']\n",
    "\n",
    "# Confidence interval using the stats.norm function\n",
    "norm_ci = stats.norm.interval(alpha=0.95,\n",
    "                             loc=analytics_df['private_equity'].mean(),\n",
    "                             scale=analytics_df['private_equity'].std() / np.sqrt(analytics_df.shape[0]))\n",
    "\n",
    "# Construct a bootstrapped confidence interval\n",
    "bootstrap_ci = stats.bootstrap(data=(analytics_df['private_equity'], ),\n",
    "                              statistic=np.mean)\n",
    "\n",
    "print('Normal CI:', norm_ci)\n",
    "print('Bootstrap CI:', bootstrap_ci.confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace84b6-deb9-4ddc-9de9-ac8e22f0f56b",
   "metadata": {},
   "source": [
    "### Fisher's method in SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bea25-b644-4a47-848d-9506437bd31a",
   "metadata": {},
   "source": [
    "##### Compute both the test statistic and the p-value for this test.\n",
    "\n",
    "##### Print out the p-value for Fisher's method.\n",
    "##### Print out if is True or False that this p-value is significant at 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae260a-b9e7-45a5-bd5a-73f2dece89d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the combined p-value and the p-value for this test\n",
    "test_statistic, p_value = stats.combine_pvalues(p_values)\n",
    "\n",
    "# Print out the p-value for the test\n",
    "print('Test p-value = ', p_value)\n",
    "\n",
    "# Print out if the p-value is signifcant at 5%\n",
    "print(p_value < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fabb3f-747f-43cf-a2a7-6b9d97a33cb0",
   "metadata": {},
   "source": [
    "### Permutation tests for correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3534e7-e587-4b01-92f4-810ff41723bd",
   "metadata": {},
   "source": [
    "##### Define a statistic() function which returns just the Pearson R value between two vectors.\n",
    "##### Set your data equal to a tuple containing the volatility of BTC and SP500.\n",
    "##### Conduct a permutation test with this data, statistic, 1000 resamples, and with an alternative hypothesis of greater volatility with Bitcoin.\n",
    "##### Print if the p-value is significant at 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244f286-7ccf-44f7-8fbb-4e42a0987b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which returns the Pearson R value\n",
    "def statistic(x, y):\n",
    "    return stats.pearsonr(x, y)[0]\n",
    "\n",
    "# Define the data as the percent daily change from each asset\n",
    "data = (btc_sp_df['Pct_Daily_Change_BTC'], btc_sp_df['Pct_Daily_Change_SP500'])\n",
    "\n",
    "# Compute a permutation test for the percent daily change of each asset\n",
    "res = stats.permutation_test(data, statistic, n_resamples=1000,\n",
    "                             vectorized=False, alternative='greater')\n",
    "\n",
    "# Print if the p-value is significant at 5%\n",
    "print(res.pvalue < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af78164-1ddb-4ae5-ab31-292078be9f48",
   "metadata": {},
   "source": [
    "### Analyzing skewed data with a permutation test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d137f0a-ca28-4f8d-9e7e-951d518b033e",
   "metadata": {},
   "source": [
    "##### Define a statistic function which, given two samples fundings_group_1 and fundings_group_2, returns the difference in mean number of funding_rounds.\n",
    "##### Conduct a permutation test using the funding_rounds column from each data set, the statistic function you defined, and 100 resamples.\n",
    "##### Print out the resulting p-value of your permutation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3691fd5-e902-4989-a325-8274daf0dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a \"statistic\" function which calculates the difference in means\n",
    "def statistic(fundings_group_1, fundings_group_2):\n",
    "  return np.mean(fundings_group_1) - np.mean(fundings_group_2)\n",
    "\n",
    "# Conduct a permutation test using 100 resamples\n",
    "perm_result = stats.permutation_test((analytics_df['funding_rounds'], non_analytics_df['funding_rounds']), \n",
    "                                      statistic=statistic,\n",
    "                                      n_resamples=100,\n",
    "                                      vectorized=False)\n",
    "\n",
    "# Print the p-value\n",
    "print(perm_result.pvalue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
